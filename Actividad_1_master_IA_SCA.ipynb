{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Actividad_1_master_IA_SCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GomezIker/master-IA-ejercicios/blob/main/Actividad_1_master_IA_SCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VFT5_VnJl19"
      },
      "source": [
        "# Actividad 1: Conceptos generales de redes neuronales\n",
        "En esta actividad vamos a revisar algunos de los conceptos basicos de las redes neuronales, pero no por ello menos importantes.\n",
        "\n",
        "El dataset a utilizar es Fashion MNIST, un problema sencillo con imágenes pequeñas de ropa, pero más interesante que el dataset de MNIST. Puedes consultar más información sobre el dataset en este enlace.\n",
        "\n",
        "El código utilizado para contestar tiene que quedar claramente reflejado en el Notebook. Puedes crear nuevas cells si así lo deseas para estructurar tu código y sus salidas. A la hora de entregar el notebook, asegúrate de que los resultados de ejecutar tu código han quedado guardados (por ejemplo, a la hora de entrenar una red neuronal tiene que verse claramente un log de los resultados de cada epoch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU8gdrsVG4h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34610b69-8b51-48b9-e398-4268a1171403"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zScMKU2OKSPD"
      },
      "source": [
        "En primer lugar vamos a importar el dataset Fashion MNIST (recordad que este es uno de los dataset de entranamiento que estan guardados en keras) que es el que vamos a utilizar en esta actividad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4voG2hxxG4h3"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JphLsCvgKrzb"
      },
      "source": [
        "Llamar a **load_data** en este dataset nos dará dos conjuntos de dos listas, estos serán los valores de entrenamiento y prueba para los gráficos que contienen las prendas de vestir y sus etiquetas.\n",
        "\n",
        "Nota: Aunque en esta actividad lo veis de esta forma, también lo vais a poder encontrar como 4 variables de esta forma: training_images, training_labels, test_images, test_labels = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1muD4PHEG4h6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b663c4-e224-48b1-c3c1-b67a0a6f50d9"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HITv-lv7R97i"
      },
      "source": [
        "#training_images.shape # 60000 imágenes de 28x28\n",
        "#training_labels.shape # 60000 etiquetas entre 0 y 9\n",
        "#test_images.shape # 10000 imágenes de 28x28\n",
        "#test_labels.shape # 10000 etiquetas entre 0 y 9"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWGpJqVVLT3Y"
      },
      "source": [
        "Antes de continuar vamos a dar un vistazo a nuestro dataset, para ello vamos a ver una imagen de entrenamiento y su etiqueta o clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5a5PlswG4h8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "88f50988-ef86-46a6-b409-3110331ff7c0"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(training_images[25], cmap=\"gray\") # recordad que siempre es preferible trabajar en blanco y negro\n",
        "#\n",
        "print(training_labels.max(), training_labels.min())\n",
        "print(training_images[25])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 0\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0  51   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 139 214 218 220 164 206 243 233 205  93   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 130 253 225 226 233 229 232 230 219 227 249  63   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 203 237 221 222 221 222 219 220 224 218 233 191   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 232 237 224 225 224 224 222 221 225 218 224 253   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 255 232 223 225 222 221 219 216 219 212 223 255  30   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   5 255 230 224 221 223 218 219 217 221 214 229 255  89   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  32 255 228 221 220 223 221 221 218 217 221 232 255 113   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  78 255 227 218 220 221 226 225 219 215 232 168 255 148   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 115 255 237 221 221 218 228 227 219 216 241 107 255 152   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 144 255 218 223 220 215 223 222 215 216 240 119 255 154   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 167 255 102 224 223 215 219 220 213 213 234 131 255 165   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 170 255  34 221 229 215 217 217 214 216 238 102 254 175   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 172 255  27 235 225 215 214 215 215 213 246 104 253 181   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 173 255  18 249 217 215 215 215 216 210 241  95 247 183   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 172 255  19 252 214 216 215 214 215 211 245 106 244 176   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 164 254  27 253 212 217 216 214 215 214 243 110 243 145   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 169 255  42 253 211 215 218 218 215 215 233 149 255 141   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0 103 131  49 253 212 216 222 219 217 214 249 128 122  78   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  58 254 218 217 225 218 219 212 253 110   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   4   0  64 237 219 220 229 217 222 217 235 129   0   3   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   3   0  54 239 221 222 231 215 225 217 237 125   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   3   0  50 241 220 224 233 212 227 217 241 120   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   3   0  41 242 222 226 236 213 228 220 243 113   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   3   0  33 242 224 228 239 216 230 221 245  97   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   3   0  13 237 224 226 235 208 226 218 246  65   0   3   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   0 217 244 245 255 253 241 236 248  22   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 115 181 103  54 141 146 134 101   0   0   1   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJvZx3MLucY"
      },
      "source": [
        "Habreis notado que todos los valores numericos están entre 0 y 255. Si estamos entrenando una red neuronal, una buena practica es transformar todos los valores entre 0 y 1, un proceso llamado \"normalización\" y afortunadamente en Python es fácil normalizar una lista. Lo puedes hacer de esta manera:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLpAvdVcRlyx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tojL1BmjG4h_"
      },
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUWWsszMAKt"
      },
      "source": [
        "Ahora vamos a definir el modelo, pero antes vamos a repasar algunos comandos y conceptos muy utiles:\n",
        "* **Sequential**: Eso define una SECUENCIA de capas en la red neuronal\n",
        "* **Dense**: Añade una capa de neuronas\n",
        "* **Flatten**: ¿Recuerdas que las imágenes cómo eran las imagenes cuando las imprimiste para poder verlas? Un cuadrado, Flatten sólo toma ese cuadrado y lo convierte en un vector de una dimensión.\n",
        "\n",
        "Cada capa de neuronas necesita una función de activación. Normalmente se usa la función relu en las capas intermedias y softmax en la ultima capa\n",
        "* **Relu** significa que \"Si X>0 devuelve X, si no, devuelve 0\", así que lo que hace es pasar sólo valores 0 o mayores a la siguiente capa de la red.\n",
        "* **Softmax** toma un conjunto de valores, y escoge el más grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgBW1yE2MwPp"
      },
      "source": [
        " **Pregunta 1 (3.5 puntos)**. Utilizando Keras, y preparando los datos de X e y como fuera necesario, define y entrena una red neuronal que sea capaz de clasificar imágenes de Fashion MNIST con las siguientes características:\n",
        "\n",
        "* Una hidden layer de tamaños 128, utilizando unidades sigmoid\n",
        "Optimizador Adam.\n",
        "* Durante el entrenamiento, la red tiene que mostrar resultados de loss y accuracy por cada epoch.\n",
        "* La red debe entrenar durante 10 epochs y batch size de 64.\n",
        "* La última capa debe de ser una capa softmax.\n",
        "* Tu red tendría que ser capaz de superar fácilmente 80% de accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTaD2QXIORwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144370ec-259a-414b-f8cc-7d2b1ac564a8"
      },
      "source": [
        "### Tu código para la red neuronal de la pregunta 1 aquí ###\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# La red se compone de 3 capas:\n",
        "# - Una capa flatten que convierte la imagen en un array lineal\n",
        "# - Una capa hidden con 128 neuronas\n",
        "# - Una capa de salida con 10 valores\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=10, batch_size=64)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.8191 - accuracy: 0.7413\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.4229 - accuracy: 0.8488\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3756 - accuracy: 0.8656\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3485 - accuracy: 0.8740\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3373 - accuracy: 0.8787\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3191 - accuracy: 0.8847\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3077 - accuracy: 0.8880\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2944 - accuracy: 0.8936\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2852 - accuracy: 0.8947\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 2s 3ms/step - loss: 0.2740 - accuracy: 0.9002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4add32cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxr5hTKYOQnK"
      },
      "source": [
        "Para concluir el entrenamiento de la red neuronal, una buena practica es evaluar el modelo para ver si la precisión de entrenamiento es real\n",
        "\n",
        "**pregunta 2 (0.5 puntos)**: evalua el modelo con las imagenes y etiquetas test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNjQEtUUG4iI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d3ca3f-5e3c-4179-baa7-a11a541ab71e"
      },
      "source": [
        "### Tu código para la evaluación de la red neuronal de la pregunta 2 aquí ###\n",
        "evaluate_loss, evaluate_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(\"Test Set Loss: \", evaluate_loss)\n",
        "print(\"Test Set Accuracy\", evaluate_acc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.3382 - accuracy: 0.8779\n",
            "Test Set Loss:  0.3382115066051483\n",
            "Test Set Accuracy 0.8779000043869019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygMVnmSYO83U"
      },
      "source": [
        "Ahora vamos a explorar el código con una serie de ejercicios para alcanzar un grado de comprensión mayor sobre las redes neuronales y su entrenamiento.\n",
        "\n",
        "# **Ejercicio 1: Funcionamiento de las predicción de la red neuronal**\n",
        "\n",
        "Para este primer ejercicio sigue los siguientes pasos: \n",
        "\n",
        "* Crea una variable llamada **classifications** para construir un clasificador para las imágenes de prueba, para ello puedes utilizar la función predict sobre el conjunto de test\n",
        "* Imprime con la función print la primera entrada en las clasificaciones. \n",
        "\n",
        "**pregunta 3.1 (0.25 puntos)**, el resultado al imprimirlo es un vector de números, \n",
        "* ¿Por qué crees que ocurre esto, y qué representa este vector de números?\n",
        "\n",
        "**pregunta 3.2 (0.25 puntos)**\n",
        "* ¿Cúal es la clase de la primera entrada#  de la variable **classifications**? La respuesta puede ser un número o su etiqueta/clase equivalente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mL-h4xQhCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41440405-c4a8-4fbf-8af6-0645699a53a9"
      },
      "source": [
        "### Tu código del clasificador de la pregunta 3 aquí ###\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.8715989e-06 1.6357993e-06 6.5315203e-06 5.0678691e-06 7.7688683e-06 2.9453743e-02 1.2742274e-05 5.7291180e-02 3.7124116e-04 9.1284615e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UOn3dg_5MUo",
        "outputId": "217ddb10-2fd5-42d6-a289-b3212b3639bd"
      },
      "source": [
        "# Comprobamos que el máximo valor está en la posición 9 del array\n",
        "print(\"Max value: \", classifications[0].max())\n",
        "print(\"Position in array: \", classifications[0].argmax())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max value:  0.91284615\n",
            "Position in array:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvbVC9gaQhMY"
      },
      "source": [
        "Tu respuesta a la pregunta 3.1 aquí:\n",
        "\n",
        "> Esto ocurre porque hemos especificado que la salida de nuestra red neuronal sea un vector de 10 valores, los cuales representan cada clase del dataset. (https://github.com/zalandoresearch/fashion-mnist#labels).\n",
        "Los valores de este vector representan la probabilidad que tiene esa imagen de pertenecer a cada uno de las 10 clases que tenemos. La máxima de ellas será la clase a la que pertenece (según el modelo). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRWo-75tdgv0"
      },
      "source": [
        "Tu respuesta a la pregunta 3.2 aquí:\n",
        "\n",
        "> La clase de la primera entrada es la clase 9, con un valor de 0.91284615, por lo que, basándonos en las etiquetas del dataset, pertenece a la clase \"Ankle Boot\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "aXJyA-3Re92g",
        "outputId": "c7d61087-5cfa-42c3-82b7-8d55557129c5"
      },
      "source": [
        "# Podemos comprobar como la imágen correspondiente a la 1ª entrada es efectivamente una \"Ankle boot\"\n",
        "plt.imshow(test_images[0], cmap='gray')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff49fc9ead0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQ8qAzhRQ4L"
      },
      "source": [
        "# **Ejercicio 2: Impacto variar el número de neuronas en las capas ocultas**\n",
        "\n",
        "En este ejercicio vamos a experimentar con nuestra red neuronal cambiando el numero de neuronas por 512 y por 1024. Para ello, utiliza la red neuronal de la pregunta 1, y su capa oculta cambia las 128 neuronas:\n",
        "\n",
        "* **pregunta 4.1 (0.25 puntos)**: 512 neuronas en la capa oculta\n",
        "* **pregunta 4.2 (0.25 puntos)**:1024 neuronas en la capa oculta\n",
        "\n",
        "y entrena la red en ambos casos.\n",
        "\n",
        "**pregunta 4.3 (0.5 puntos)**: ¿Cual es el impacto que tiene la red neuronal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdP8ZwuaUV93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9127b3-03f7-40c5-c242-c76951e34f4d"
      },
      "source": [
        "### Tu código para 512 neuronas aquí ###\n",
        "model_2 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(512, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_2.fit(training_images, training_labels, epochs=10, batch_size=64)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.7007 - accuracy: 0.7598\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.4181 - accuracy: 0.8492\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.3678 - accuracy: 0.8673\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.3365 - accuracy: 0.8789\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.3137 - accuracy: 0.8866\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.3015 - accuracy: 0.8873\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.2843 - accuracy: 0.8957\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.2651 - accuracy: 0.9026\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.2562 - accuracy: 0.9056\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.2445 - accuracy: 0.9091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff49f4767d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXBlbbfuUaPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86db9ade-7259-4f6d-9d83-a419a1eacecf"
      },
      "source": [
        "### Tu código para 1024 neuronas aquí ###\n",
        "model_3 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(1024, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_3.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_3.fit(training_images, training_labels, epochs=10, batch_size=64)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.6665 - accuracy: 0.7637\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.4164 - accuracy: 0.8509\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.3689 - accuracy: 0.8656\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.3384 - accuracy: 0.8761\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.3081 - accuracy: 0.8868\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.2944 - accuracy: 0.8899\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.2778 - accuracy: 0.8968\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.2636 - accuracy: 0.9022\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.2512 - accuracy: 0.9066\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.2367 - accuracy: 0.9093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff49f46cc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saXBRv6Cij3S",
        "outputId": "8ff72546-9328-47f2-eb2c-ac7117344374"
      },
      "source": [
        "model.summary()\n",
        "model_2.summary()\n",
        "model_3.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 814,090\n",
            "Trainable params: 814,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG0h2HL-Uj93"
      },
      "source": [
        "Tu respuesta a la pregunta 4.3 aquí:\n",
        "\n",
        "> El impacto inmediato que se observa es que el tiempo de entrenamiento aumenta considerablemente (casi doblando el tiempo de ejecución de cada epoch).\n",
        "Por otro lado, vemos que al estudiar el summary() de cada red, pasamos de 101.770 parámetros, a 407.050 en la de 512 y 814.090 en la de 1024.\n",
        "También vemos que el accuracy mejora unos pocos puntos en cada entrenamiento, segun vamos aumentando el número de neuronas, llegando a un accuracy del 90,91% en la red de 512 neuronas y un accuracy de 91,93% en la de 1024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-NpUI9EVkVz"
      },
      "source": [
        "Si ahora entrenais el modelo de esta forma (con 512 y 1024 neuronas en la capa oculta) y volveis a ejecutar el predictor guardado en la variable **classifications**, escribir el código del clasificador del ejercicio 1 de nuevo e imprimid el primer objeto guardado en la variable classifications.\n",
        "\n",
        "**pregunta 5.1 (0.25 puntos)**: \n",
        "\n",
        "* ¿En que clase esta clasificado ahora la primera prenda de vestir de la variable classifications?\n",
        "\n",
        "**pregunta 5.1 (0.25 puntos)**: \n",
        "\n",
        "* ¿Porque crees que ha ocurrido esto?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdJHl3V-G4iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3e3171-8041-45b2-95ce-312da673421a"
      },
      "source": [
        "### Tu código del clasificador de la pregunta 5 aquí ###\n",
        "classifications_2 = model_2.predict(test_images)\n",
        "classifications_3 = model_3.predict(test_images)\n",
        "\n",
        "# Etiqueta del primer clasificador\n",
        "print(\"Clasificador con 128 neuronas: \",classifications[0].argmax())\n",
        "\n",
        "# Etiqueta del segundo\n",
        "print(\"Clasificador con 512 neuronas: \",classifications_2[0].argmax())\n",
        "\n",
        "# Etiqueta del tercero\n",
        "print(\"Clasificador con 1024 neuronas: \",classifications_3[0].argmax())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clasificador con 128 neuronas:  9\n",
            "Clasificador con 512 neuronas:  9\n",
            "Clasificador con 1024 neuronas:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3NfwdOGZcAa"
      },
      "source": [
        "Tu respuesta a la pregunta 5.1 aquí:\n",
        "\n",
        "> En este caso, tanto el modelo de 512 como el de 1024 predicen la misma clase que el modelo de 128 (La clase 9, o \"Ankle boot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFmfpxE1ZcJx"
      },
      "source": [
        "Tu respuesta a la pregunta 5.2 aquí:\n",
        "\n",
        "> Al incluir más neuronas en la capa oculta, la predicción de ambos modelos debería mejorar. Y así es, como vemos, ambas mantienen la misma predicción que el modelo original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59eM76O1YekZ"
      },
      "source": [
        "# **Ejercicio 3: ¿por qué es tan importante la capa Flatten?**\n",
        "\n",
        "En este ejercicio vamos a ver que ocurre cuando quitamos la capa flatten, para ello, escribe la red neuronal de la pregunta 1 y no pongas la capa Flatten.\n",
        "\n",
        "**pregunta 6 (0.5 puntos):** ¿puedes explicar porque da el error que da?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecfEVKEuG4iU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "07a524fd-5019-4a76-ec1e-e5cb4d133b31"
      },
      "source": [
        "### Tu código de la red neuronal sin capa flatten de la pregunta 6 aquí ###\n",
        "model_4 = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_4.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_4.fit(training_images, training_labels, epochs=10, batch_size=64)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ab226c15b62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  assertion failed: [Condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [64 1] [y (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [64 28]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert (defined at <ipython-input-17-ab226c15b62b>:13) ]] [Op:__inference_train_function_60724]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aNmrkkOZN6D"
      },
      "source": [
        "Tu respuesta a la pregunta 6 aquí:\n",
        "\n",
        "> Al quitar la capa flatten, la capa de entrada de la red neuronal pasa a ser una capa con 128 neuronas (por lo que espera 128 valores), sin embargo, le estamos pasando imágenes (matrices) de 28x28, por lo que no sabe cómo actuar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37cIr81ZYJj"
      },
      "source": [
        "# **Ejercicio 4: Número de neuronas de la capa de salida**\n",
        "Considerad la capa final, la de salida de la red neuronal de la pregunta 1.\n",
        "\n",
        "**pregunta 7.1 (0.25 puntos)**: ¿Por qué son 10 las neuronas de la última capa?\n",
        "\n",
        "**pregunta 7.2 (0.25 puntos)**: ¿Qué pasaría si tuvieras una cantidad diferente a 10? \n",
        "\n",
        "Por ejemplo, intenta entrenar la red con 5, para ello utiliza la red neuronal de la pregunta 1 y cambia a 5 el número de neuronas en la última capa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhbZkppYZOCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "e0c040dc-22cb-4439-d91f-b1b142a3a0f5"
      },
      "source": [
        "### Tu código de la red neuronal con 5 neuronas en la capa de salida de la pregunta 7 aquí ###\n",
        "model_5 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='sigmoid'),\n",
        "    keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model_5.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_5.fit(training_images, training_labels, epochs=10, batch_size=64)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e67559ab9252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 3 4 7 9 0 3 3 9 8 4 7 7 2 0 4 1 6 5 0 3 1 6 0 8 8 4 6 4 8 3 7 7 5 9 8 8 4 2 0 6 4 6 2 4 6 3 2 1 6 6 4 0 4 7 3 6 4 4 2 4 8 6 9 3\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-18-e67559ab9252>:14) ]] [Op:__inference_train_function_61267]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLsQcq-6aUoD"
      },
      "source": [
        "Tu respuestas a la pregunta 7.1 aquí:\n",
        "\n",
        "> Porque son 10 las etiquetas del dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1f_7ZFeaUu6"
      },
      "source": [
        "Tu respuestas a la pregunta 7.2 aquí:\n",
        "\n",
        "> Nos devuelve un error de Argumento Inválido, debido a que el conjunto de entrenamiento tiene 10 etiquetas (de 0 a 9), pero el modelo entrenado, solo puede devolver 5 valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNIBCkshaf2y"
      },
      "source": [
        "# Ejercicio 5: Aumento de epoch y su efecto en la red neuronal\n",
        "En este ejercicio vamos a ver el impacto de aumentar los epoch en el entrenamiento. Usando la red neuronal de la pregunta 1:\n",
        "\n",
        "**pregunta 8.1 (0.20 puntos)**\n",
        "* Intentad 15 epoch para su entrenamiento, probablemente obtendras un modelo con una pérdida mucho mejor que el que tiene 5.\n",
        "\n",
        "**pregunta 8.2 (0.20 puntos)**\n",
        "* Intenta ahora con 30 epoch para su entrenamiento, podrás ver que el valor de la pérdida deja de disminuir, y a veces aumenta.\n",
        "\n",
        "**pregunta 8.3 (0.60 puntos)**\n",
        "* ¿Porque que piensas que ocurre esto? Explica tu respuesta y da el nombre de este efecto si lo conoces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5vk_imG4iZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b952af-4cdb-4a16-fe0b-50d3cd589098"
      },
      "source": [
        "### Tu código para 15 epoch aquí ###\n",
        "model_6 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_6.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_6.fit(training_images, training_labels, epochs=15, batch_size=64)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.8327 - accuracy: 0.7357\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.4260 - accuracy: 0.8488\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3839 - accuracy: 0.8634\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3584 - accuracy: 0.8725\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3358 - accuracy: 0.8781\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3160 - accuracy: 0.8866\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3074 - accuracy: 0.8896\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2917 - accuracy: 0.8923\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2790 - accuracy: 0.8977\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2783 - accuracy: 0.8979\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2636 - accuracy: 0.9029\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2599 - accuracy: 0.9046\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2496 - accuracy: 0.9081\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2428 - accuracy: 0.9104\n",
            "Epoch 15/15\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2340 - accuracy: 0.9133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff499f52490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9jQ26Gda5cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe8a8f1-eb71-450b-dc82-99a0cdbeafbc"
      },
      "source": [
        "### Tu código para 30 epoch aquí ###\n",
        "model_7 = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_7.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_7.fit(training_images, training_labels, epochs=30, batch_size=64)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.8264 - accuracy: 0.7450\n",
            "Epoch 2/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.4226 - accuracy: 0.8489\n",
            "Epoch 3/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3763 - accuracy: 0.8661\n",
            "Epoch 4/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3493 - accuracy: 0.8741\n",
            "Epoch 5/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3291 - accuracy: 0.8793\n",
            "Epoch 6/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3118 - accuracy: 0.8867\n",
            "Epoch 7/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2998 - accuracy: 0.8913\n",
            "Epoch 8/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2937 - accuracy: 0.8934\n",
            "Epoch 9/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2818 - accuracy: 0.8972\n",
            "Epoch 10/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2703 - accuracy: 0.9017\n",
            "Epoch 11/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2650 - accuracy: 0.9018\n",
            "Epoch 12/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2571 - accuracy: 0.9049\n",
            "Epoch 13/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2476 - accuracy: 0.9103\n",
            "Epoch 14/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2383 - accuracy: 0.9123\n",
            "Epoch 15/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2317 - accuracy: 0.9141\n",
            "Epoch 16/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2294 - accuracy: 0.9156\n",
            "Epoch 17/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2231 - accuracy: 0.9181\n",
            "Epoch 18/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2180 - accuracy: 0.9198\n",
            "Epoch 19/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2110 - accuracy: 0.9233\n",
            "Epoch 20/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2086 - accuracy: 0.9236\n",
            "Epoch 21/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2006 - accuracy: 0.9285\n",
            "Epoch 22/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1963 - accuracy: 0.9278\n",
            "Epoch 23/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1950 - accuracy: 0.9293\n",
            "Epoch 24/30\n",
            "938/938 [==============================] - 2s 3ms/step - loss: 0.1850 - accuracy: 0.9327\n",
            "Epoch 25/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1824 - accuracy: 0.9349\n",
            "Epoch 26/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1737 - accuracy: 0.9372\n",
            "Epoch 27/30\n",
            "938/938 [==============================] - 2s 3ms/step - loss: 0.1733 - accuracy: 0.9367\n",
            "Epoch 28/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1729 - accuracy: 0.9378\n",
            "Epoch 29/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1643 - accuracy: 0.9415\n",
            "Epoch 30/30\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1642 - accuracy: 0.9415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff497e533d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0fjzH4bmSR"
      },
      "source": [
        "Tu respuesta a la pregunta 8.3 aquí:\n",
        "\n",
        "> AL intentar minimizar la función de perdida, es probable que el modelo haya llegado a un mínimo local, produciendo esos \"saltos\". A este efecto se le llama over-fitting, significa que el modelo ha aprendido muy bien sobre los datos de entrada, pero no \"generaliza\" haciendo que la precisión no sea tan buena cuando le pasamos datos nuevos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlIgNG4Yb_N6"
      },
      "source": [
        "# Ejercicio 6: Early stop\n",
        "En el ejercicio anterior, cuando entrenabas con epoch extras, tenías un problema en el que tu pérdida podía cambiar. Puede que te haya llevado un poco de tiempo esperar a que el entrenamiento lo hiciera,  y puede que hayas pensado \"¿no estaría bien si pudiera parar el entrenamiento cuando alcance un valor deseado?\", es decir, una precisión del 85% podría ser suficiente para ti, y si alcanzas eso después de 3 epoch, ¿por qué sentarte a esperar a que termine muchas más épocas? Como cualquier otro programa existen formas de parar la ejecución\n",
        "\n",
        "A partir del ejemplo de código que se da, hacer una nueva función que tenga en cuenta la perdida (loss) y que pueda parar el código para\n",
        "evitar que ocurra el efeto secundario que vimos en el ejercicio 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5UwceFUG4ic"
      },
      "source": [
        "### Ejemplo de código\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy')> 0.85):\n",
        "              print(\"\\nAlcanzado el 85% de precisión, se cancela el entrenamiento!!\")\n",
        "              self.model.stop_training = True"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bjd8wGKccrn"
      },
      "source": [
        "**Pregunta 9 (2 puntos)**: Completa el siguiente código con una clase callback que una vez alcanzado el 40% de perdida detenga el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29LSfdOvc270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf7be0e-6412-4bde-f5f3-1faf4b27fcc5"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "### Tu código de la función callback para parar el entrenamiento de la red neuronal al 40% de loss aqui: ###\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('loss') < 0.4):\n",
        "              print(\"\\nAlcanzado el 40% de pérdida, se cancela el entrenamiento!!\")\n",
        "              self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) \n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.5920 - accuracy: 0.7914\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3699 - accuracy: 0.8666\n",
            "\n",
            "Alcanzado el 40% de pérdida, se cancela el entrenamiento!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff49755d050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}